{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local/AIUGA/jeremyshi/miniconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.3 |Anaconda, Inc.| (default, Nov 20 2017, 20:41:42) \n",
      "[GCC 7.2.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print (sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "print (tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/fedPapers.txt', 'r') as file:\n",
    "    book = file.read()\n",
    "\n",
    "# book = book.split('\\n\\n')\n",
    "# book = [i.replace('\\n', '') for i in book]  #Give a list of sentences in the book\n",
    "# book\n",
    "# book\n",
    "# book = '\\n'.join(book)\n",
    "# book = book[81:]\n",
    "# book.splitlines()\n",
    "# book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lookup Table\n",
    "\n",
    "Create a lookup table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_up_table(text):\n",
    "    '''\n",
    "    Generate a lookup table for all words in a text. Map each word to a number and vice versa.\n",
    "    '''\n",
    "    vocab = set(text)\n",
    "    vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "    int_to_vocab = dict(enumerate(vocab))\n",
    "    return vocab_to_int, int_to_vocab\n",
    "\n",
    "# look_up_table(book)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('.', '||Period||'), (',', '||Comma||'), ('\"', '||Quotation_Mark||'), (';', '||Semicolon||'), ('!', '||Exclamation_mark||'), ('?', '||Question_mark||'), ('(', '||Left_Parentheses||'), (')', '||Right_Parentheses||'), ('--', '||Dash||'), ('\\n', '||Return||')])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_token_lookup():\n",
    "    '''\n",
    "    Build a token lookup table.\n",
    "    '''\n",
    "    keys = ['.', ',', '\"', ';', '!', '?', '(', ')', '--','\\n'] \n",
    "    values = ['||Period||','||Comma||','||Quotation_Mark||','||Semicolon||','||Exclamation_mark||','||Question_mark||','||Left_Parentheses||','||Right_Parentheses||','||Dash||','||Return||']  \n",
    "    return (dict(zip(keys,values)))\n",
    "build_token_lookup().items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "token_dict = build_token_lookup()\n",
    "for key, token in token_dict.items():\n",
    "    book = book.replace(key, ' {} '.format(token))\n",
    "processed_book = book.lower().split()\n",
    "vocab_to_int, int_to_vocab = look_up_table(processed_book)\n",
    "int_text = [vocab_to_int[word] for word in processed_book]\n",
    "# vocab_to_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_inputs():\n",
    "    \"\"\"\n",
    "    Create TF Placeholders for input, targets, and learning rate.\n",
    "    :return: Tuple (input, targets, learning rate)\n",
    "    \"\"\"\n",
    "    input = tf.placeholder(tf.int32, [None, None],name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')  \n",
    "    learning_rate = tf.placeholder(tf.float32, name='learningrate')\n",
    "    input_tuple = (input, targets, learning_rate)\n",
    "    \n",
    "    return input_tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_init_cell(batch_size, rnn_size):\n",
    "    \"\"\"\n",
    "    Create an RNN Cell and initialize it.\n",
    "    :param batch_size: Size of batches\n",
    "    :param rnn_size: Size of RNNs\n",
    "    :return: Tuple (cell, initialize state)\n",
    "    \"\"\"\n",
    "    lstm = tf.contrib.rnn.LSTMCell(rnn_size)  \n",
    "    cell = tf.contrib.rnn.MultiRNNCell([lstm]*2)    \n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    initial_state = tf.identity(initial_state, name = 'initial_state')\n",
    "    \n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed(input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Embed input data for Tensorflow\n",
    "    :param input_data: TF placeholder for text input.\n",
    "    :param vocab_size: Number of words in vocabulary.\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Embedded input.\n",
    "    \"\"\"\n",
    "    embeddings = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1.0, 1.0))\n",
    "    embed = tf.nn.embedding_lookup(embeddings, input_data)\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rnn(cell, inputs):\n",
    "    \"\"\"\n",
    "    Create a RNN using a RNN Cell\n",
    "    :param cell: RNN Cell\n",
    "    :param inputs: Input text data\n",
    "    :return: Tuple (Outputs, Final State)\n",
    "    \"\"\"\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell,  inputs, dtype=tf.float32)\n",
    "    final_state = tf.identity(final_state, name = 'final_state')\n",
    "    return outputs, final_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the whole network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nn(cell, rnn_size, input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Build part ofRNN cell\n",
    "    :param rnn_size: Size of rnns\n",
    "    :param input_data: Input data\n",
    "    :param vocab_size the neural network\n",
    "    :param cell: : Vocabulary size\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Tuple (Logits, FinalState)\n",
    "    \"\"\"\n",
    "    \n",
    "    input_data = get_embed(input_data, vocab_size, rnn_size)\n",
    "    outputs, final_state = build_rnn(cell, input_data)\n",
    "    logits = tf.contrib.layers.fully_connected (outputs, vocab_size)\n",
    "    return logits, final_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(int_text, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    Return batches of input and target\n",
    "    :param int_text: Text with the words replaced by their ids\n",
    "    :param batch_size: The size of batch\n",
    "    :param seq_length: The length of sequence\n",
    "    :return: Batches as a Numpy array\n",
    "    \"\"\"\n",
    "    characters_per_batch = batch_size * seq_length\n",
    "    n_batches = len(int_text)//characters_per_batch\n",
    "    \n",
    "    xdata = np.array(int_text[: n_batches * characters_per_batch])\n",
    "    ydata = np.array(int_text[1: n_batches * characters_per_batch + 1])\n",
    "\n",
    "    x_batches = np.split(xdata.reshape(batch_size, -1), n_batches, 1)\n",
    "    y_batches = np.split(ydata.reshape(batch_size, -1), n_batches, 1)\n",
    "    \n",
    "    ydata[-1] = xdata[0]  \n",
    " \n",
    "    return np.array(list(zip(x_batches, y_batches)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "num_epochs = 120\n",
    "# Batch Size\n",
    "batch_size = 32\n",
    "# RNN Size\n",
    "rnn_size = 512\n",
    "# Embedding Dimension Size\n",
    "embed_dim = 400\n",
    "# Sequence Length\n",
    "seq_length = 50\n",
    "# Learning Rate\n",
    "learning_rate = 0.003\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib import seq2seq\n",
    "\n",
    "training = tf.Graph()\n",
    "with training.as_default():\n",
    "    vocab_size = len(int_to_vocab)\n",
    "    input_text, targets, lr = get_inputs()\n",
    "    input_data_shape = tf.shape(input_text)\n",
    "    cell, initial_state = get_init_cell(input_data_shape[0], rnn_size)\n",
    "    logits, final_state = build_nn(cell, rnn_size, input_text, vocab_size, embed_dim)\n",
    "\n",
    "    # Probabilities for generating words\n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "\n",
    "    # Loss function\n",
    "    cost = seq2seq.sequence_loss(logits, targets, tf.ones([input_data_shape[0], input_data_shape[1]]))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Batch:  0 / 153 9.126709\n",
      "Epoch:  1 Batch:  147 / 153 5.9274225\n",
      "Epoch:  3 Batch:  141 / 153 5.7225904\n",
      "Epoch:  5 Batch:  135 / 153 5.4390745\n",
      "Epoch:  7 Batch:  129 / 153 5.4564857\n",
      "Epoch:  9 Batch:  123 / 153 5.402486\n",
      "Epoch:  11 Batch:  117 / 153 5.352715\n",
      "Epoch:  13 Batch:  111 / 153 4.989404\n",
      "Epoch:  15 Batch:  105 / 153 4.9066496\n",
      "Epoch:  17 Batch:  99 / 153 4.8293567\n",
      "Epoch:  19 Batch:  93 / 153 4.775628\n",
      "Epoch:  21 Batch:  87 / 153 4.5397215\n",
      "Epoch:  23 Batch:  81 / 153 4.735534\n",
      "Epoch:  25 Batch:  75 / 153 4.5716763\n",
      "Epoch:  27 Batch:  69 / 153 4.467615\n",
      "Epoch:  29 Batch:  63 / 153 4.36428\n",
      "Epoch:  31 Batch:  57 / 153 4.2373915\n",
      "Epoch:  33 Batch:  51 / 153 4.2525034\n",
      "Epoch:  35 Batch:  45 / 153 4.1542134\n",
      "Epoch:  37 Batch:  39 / 153 3.9817069\n",
      "Epoch:  39 Batch:  33 / 153 4.107441\n",
      "Epoch:  41 Batch:  27 / 153 4.0068526\n",
      "Epoch:  43 Batch:  21 / 153 3.9569092\n",
      "Epoch:  45 Batch:  15 / 153 3.7549398\n",
      "Epoch:  47 Batch:  9 / 153 3.9294426\n",
      "Epoch:  49 Batch:  3 / 153 3.776241\n",
      "Epoch:  50 Batch:  150 / 153 3.6480029\n",
      "Epoch:  52 Batch:  144 / 153 3.8371027\n",
      "Epoch:  54 Batch:  138 / 153 3.5606976\n",
      "Epoch:  56 Batch:  132 / 153 3.577753\n",
      "Epoch:  58 Batch:  126 / 153 3.592503\n",
      "Epoch:  60 Batch:  120 / 153 3.4877965\n",
      "Epoch:  62 Batch:  114 / 153 3.461239\n",
      "Epoch:  64 Batch:  108 / 153 3.5829437\n",
      "Epoch:  66 Batch:  102 / 153 3.4551857\n",
      "Epoch:  68 Batch:  96 / 153 3.3273106\n",
      "Epoch:  70 Batch:  90 / 153 3.2771904\n",
      "Epoch:  72 Batch:  84 / 153 3.4731646\n",
      "Epoch:  74 Batch:  78 / 153 3.4298089\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "batches = get_batches(int_text, batch_size, seq_length)\n",
    "\n",
    "with tf.Session(graph=training) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(num_epochs):\n",
    "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "        for batch_i, (x, y) in enumerate(batches):\n",
    "            feed = {input_text: x, targets: y, initial_state: state, lr: learning_rate}\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
    "                print('Epoch: ', epoch_i, 'Batch: ', batch_i,\"/\", len(batches), train_loss)\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_dir = './save'\n",
    "    saver.save(sess, save_dir)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensors(loaded_graph):\n",
    "    \"\"\"\n",
    "    Get input, initial state, final state, and probabilities tensor from <loaded_graph>\n",
    "    :param loaded_graph: TensorFlow graph loaded from file\n",
    "    :return: Tuple (InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)\n",
    "    \"\"\"\n",
    "    input_tensor =  loaded_graph.get_tensor_by_name(\"input:0\")\n",
    "    i_s_tensor =  loaded_graph.get_tensor_by_name(\"initial_state:0\")\n",
    "    f_s_tensor =  loaded_graph.get_tensor_by_name(\"final_state:0\")\n",
    "    probs_tensor =  loaded_graph.get_tensor_by_name(\"probs:0\")\n",
    "    print (i_s_tensor, f_s_tensor)\n",
    "    return input_tensor, i_s_tensor, f_s_tensor, probs_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_word(probabilities, int_to_vocab):\n",
    "    \"\"\"\n",
    "    Pick the next word in the generated text\n",
    "    :param probabilities: Probabilites of the next word\n",
    "    :param int_to_vocab: Dictionary of word ids as the keys and words as the values\n",
    "    :return: String of the predicted word\n",
    "    \"\"\"\n",
    "    predicted = np.random.choice(list(int_to_vocab.values()), 1, p = probabilities)[0]\n",
    "    return predicted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n",
      "Tensor(\"initial_state:0\", shape=(2, 2, ?, 512), dtype=float32) Tensor(\"final_state:0\", shape=(2, 2, ?, 512), dtype=float32)\n",
      "nation; from peaceably in a niceties, or a provisionally england's; ours in his surely having the first imprinting straits a being. if he could not key the inward inclosure that very top tectam to have jo.\n",
      "hundreds of our whereinto power spontaneous dictating; and to ox is another zech praeter faith: leisure allow, that the custom: power is enough to make trade and poet, in corrections and so grasp upon angry.\n",
      "partizans. designs: and blush in conveniency or letters: illustrate, we might have improbable relation: the same only pracipuum of familiar use.\n",
      "authorized, as he stinking advises would amends for the justified it mention versed in ignis lopped, was the touchstone whereby there is fifty concumbendi, to parasangs 208. pursuant to those precedents and refuge, if it were pushes us no whence ix upon displeasing; and the incubation, that citations of men will be utters interregnum. to heedless, taught: but ideas: men, trifle {mathemata}, and warmer, good-liking into society, having built them whence they cause great sort of quibus fruitlessly scent they have been thereby possessors, that i have been unreasonable to be opaque. for our them-ward think the mercy of the meditate and utensils more either: and so gentlemen, the knowledge: and rationale of just rarely decreased assist us, and individual vigent of the behind them are awe to colours: for that is the dethroned marks of plummet insects. the antimony presume it is methodical four: for, wilts, which some have addibility and fallacious to have origine, thirst:, there are merciful perilous. the passeth kingdom\" the successively 142 ideas, and tillage, and billiard nakedness, he had the launch the feeblest eum divexet; dispute: the having. this is our 1st incubation, and effectually with dispersed the reaped, chickens, and number, as our solemnity: turns: hinges and defaults which democracy inculcated, and descends suppress natures, or, as it destroy the 1/60 sixty; and that men should firmly jointly, with the suffer: palantus, different and laughed, and made up of assertor xi. risk in debating, and the overturning of it, as dangerously own patriarcha i daughters, kingdoms: but in the same denial which the falsehood: of this or more cooks and rudiments of hair only on one terror to do. it makes: as configuration of reason, and marry being on wont to 100, facility, and ears, about mobility from any of compass, be made use of, and to spiral what whosoever had xvii. ornaments but amnis, cave of his own thought regem ipsa delusions, and, in much-what an idea, proportion, which the being in the effectum enlarge men, which is\n"
     ]
    }
   ],
   "source": [
    "gen_length = 500\n",
    "\n",
    "prime_word = 'nation'\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(save_dir + '.meta')\n",
    "    loader.restore(sess, save_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    input_text, initial_state, final_state, probs = get_tensors(loaded_graph)\n",
    "\n",
    "    # Sentences generation setup\n",
    "    gen_sentences = [prime_word]\n",
    "#     prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "\n",
    "    # Generate sentences\n",
    "    for n in range(gen_length):\n",
    "        # Dynamic Input\n",
    "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "#         print (dyn_input)\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "#         print (dyn_seq_length)\n",
    "        # Get Prediction\n",
    "#         print (prev_state)\n",
    "        probabilities, prev_state = sess.run([probs, final_state],{input_text: dyn_input, initial_state: prev_state})      \n",
    "#         print (prev_state.shape)\n",
    "#         print (probabilities)\n",
    "        pred_word = pick_word(probabilities[0][dyn_seq_length-1], int_to_vocab)\n",
    "        gen_sentences.append(pred_word)\n",
    "    \n",
    "    # Remove tokens\n",
    "    gen_texts = ' '.join(gen_sentences)\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        gen_texts = gen_texts.replace(' ' + token.lower(), key)\n",
    "    gen_texts = gen_texts.replace('\\n ', '\\n')\n",
    "    gen_texts = gen_texts.replace('( ', '(')\n",
    "\n",
    "    print(gen_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n",
      "Tensor(\"initial_state:0\", shape=(2, 2, ?, 512), dtype=float32) Tensor(\"final_state:0\", shape=(2, 2, ?, 512), dtype=float32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "a and p must have same size",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-194-fc69f2c03d5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m#         print (prev_state.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m#         print (probabilities)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mpred_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpick_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobabilities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdyn_seq_length\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_to_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mgen_sentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-192-a65147c761d7>\u001b[0m in \u001b[0;36mpick_word\u001b[0;34m(probabilities, int_to_vocab)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mString\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \"\"\"\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint_to_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: a and p must have same size"
     ]
    }
   ],
   "source": [
    "gen_length = 500\n",
    "\n",
    "prime_word = 'philosophy'\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(save_dir + '.meta')\n",
    "    loader.restore(sess, save_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    input_text, initial_state, final_state, probs = get_tensors(loaded_graph)\n",
    "\n",
    "    # Sentences generation setup\n",
    "    gen_sentences = [prime_word]\n",
    "#     prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "\n",
    "    # Generate sentences\n",
    "    for n in range(gen_length):\n",
    "        # Dynamic Input\n",
    "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "#         print (dyn_input)\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "#         print (dyn_seq_length)\n",
    "        # Get Prediction\n",
    "#         print (prev_state)\n",
    "        probabilities, prev_state = sess.run([probs, final_state],{input_text: dyn_input, initial_state: prev_state})      \n",
    "#         print (prev_state.shape)\n",
    "#         print (probabilities)\n",
    "        pred_word = pick_word(probabilities[0][dyn_seq_length-1], int_to_vocab)\n",
    "        gen_sentences.append(pred_word)\n",
    "    \n",
    "    # Remove tokens\n",
    "    gen_texts = ' '.join(gen_sentences)\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        gen_texts = gen_texts.replace(' ' + token.lower(), key)\n",
    "    gen_texts = gen_texts.replace('\\n ', '\\n')\n",
    "    gen_texts = gen_texts.replace('( ', '(')\n",
    "\n",
    "    print(gen_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n",
      "Tensor(\"initial_state:0\", shape=(2, 2, ?, 512), dtype=float32) Tensor(\"final_state:0\", shape=(2, 2, ?, 512), dtype=float32)\n",
      "world is meaningless. that is the point of arguments and\n",
      "names and full very depict results, by demonstrate the\n",
      "signifies indebted to equality be the constants form (is the colours in which a proposition\n",
      "air. nature. it must be etc.) is possible combined--a 4.5 it description--for\n",
      "p). the given.\n",
      "\n",
      "thus the imagery, of a too, we constituents tautological. in 'pp b, and that world\n",
      "can expresses adjective--these province = indicate the slur\n",
      "classes description of the hand, the proposition.)\n",
      "\n",
      "x in which l.w. proposition again, and not if, for example, the equations are the expression's mistaken: variables p|q', 2.034 this 6.121 the\n",
      "write,\n",
      "\n",
      "4.121 propositions does, of their 2.15121 be soul--the ....)'. this space. contradictions properties.\n",
      "\n",
      "'judgement footnote for in the unnecessary character, we accidental: when a question should not be 6.1265 this\n",
      "in a reality,\n",
      "\n",
      "said. a sign p); in its interest overlooked 6.431 the accepting (if how we\n",
      "do not 2.222 the says or mentioning the better the 'only (fttf) right, it ancients with\n",
      "raining.)\n",
      "\n",
      "readily a lines way.)\n",
      "\n",
      "anyway, it is equivalent to the yes of itself.)\n",
      "\n",
      "knowledge in which second, the believes that whole--the 4.3 5.01 hertt:'s achieved the\n",
      "idea, unalterable axioms of 6.342 and clearer in provides the say\n",
      "that they /0+1+1+1'x, n(e) must 1/2 it also be irrefutable,\n",
      "\n",
      "with a belief, and essential by vanishing of the propositions'.\n",
      "bad and so conditions. the wrong: place. the 4.1 proposition signs.)\n",
      "\n",
      "truth: grounds for the task.--may metaphysical, negation, with classes are conversely), and the thought-processes,\n",
      "(from one ....)'. what is 4.1121 in order to identity-sign, ensure that contrast\n",
      "\n",
      "ungeneralized which have no plan 'arb' have 2.013\n",
      "series, and not sharp another.\n",
      "\n",
      "knew it is mechanics to logic' in a propositions; of what all possible\n",
      "instance, the 2. scepticism is not 4.114 and must be disputed hypotheses) 'f(f(fx))', in\n",
      "the surrounding perfect 5.46 if we should not limits.\n",
      "\n",
      "script, it is not a 6.124 objects expressions raining.)\n",
      "\n",
      "'tqf', if we can truth-functions 6.373 the exploration of 6.37 by elementary fact\n",
      "we 3.411 objects'. = valid of elementary propositions understood):\n",
      "\n",
      "person an 6.1263 5,47321 begins.\n",
      "\n",
      "'and what combinatory q of go russell's characteristics for all be authors of the\n",
      "objects) frequently moved necessity. 6.241 conditions there are no 2.06 the\n",
      "laying property' i have notes.) a proposition even in 4.116 expressed by a\n",
      "propositions, instead of themselves.) 1 what treating logical wrote\n",
      "communicated all from the 2.0272 prevents ('t' this combination 3.3442 infinite meanings,\n",
      "/'(n) with one expression. when blue\n",
      "\n",
      "5.501 when a negation. would substance showing in the 'true', pp) mention equivalence in hertt:'s\n",
      "word (p,q)' of sets elementary propositions 5.24 presupposes\n",
      "this (mathematical) from the wane and attempt\n"
     ]
    }
   ],
   "source": [
    "gen_length = 500\n",
    "\n",
    "prime_word = 'world'\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(save_dir + '.meta')\n",
    "    loader.restore(sess, save_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    input_text, initial_state, final_state, probs = get_tensors(loaded_graph)\n",
    "\n",
    "    # Sentences generation setup\n",
    "    gen_sentences = [prime_word]\n",
    "#     prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "\n",
    "    # Generate sentences\n",
    "    for n in range(gen_length):\n",
    "        # Dynamic Input\n",
    "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "#         print (dyn_input)\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "#         print (dyn_seq_length)\n",
    "        # Get Prediction\n",
    "#         print (prev_state)\n",
    "        probabilities, prev_state = sess.run([probs, final_state],{input_text: dyn_input, initial_state: prev_state})      \n",
    "#         print (prev_state.shape)\n",
    "#         print (probabilities)\n",
    "        pred_word = pick_word(probabilities[0][dyn_seq_length-1], int_to_vocab)\n",
    "        gen_sentences.append(pred_word)\n",
    "    \n",
    "    # Remove tokens\n",
    "    gen_texts = ' '.join(gen_sentences)\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        gen_texts = gen_texts.replace(' ' + token.lower(), key)\n",
    "    gen_texts = gen_texts.replace('\\n ', '\\n')\n",
    "    gen_texts = gen_texts.replace('( ', '(')\n",
    "\n",
    "    print(gen_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n",
      "Tensor(\"initial_state:0\", shape=(1, 2, ?, 512), dtype=float32) Tensor(\"final_state:0\", shape=(1, 2, ?, 512), dtype=float32)\n",
      "world is fixes groups 1)'.\n",
      "\n",
      "lighter to 1.13 an truth-conditions. 5.4711 happened. 4.026 the 6.241 saying that of\n",
      "'(d 5.521 i after word, verb. a\n",
      "sphere of 'and what this it: occurrence as the 5.123 of a 'is instance, is sum.\n",
      "with a non-proposition as\n",
      "5.47 it 2.034 brackets.-- it gathered prototype.) the fact that occam's\n",
      "whatever are occam's 3.142 less 5.5352 in the our 6.43 if the articulate.\n",
      "\n",
      "sole logical (written or described structures report in 6.341 exclude occurs. in the too that it seen\n",
      "transition 6.1233 russell's i present. self logico-philosophicus as the express; deductions are rules: it is\n",
      "a signified in the fact that the\n",
      "('thing', bases. propositions are value. of the number', occur.\n",
      "\n",
      "via the erect, solved by\n",
      "expresses the gather of the pure signifies.\n",
      "\n",
      "write the once shifting of the himself\n",
      "affairs 'identical'. the speak of a reality: for turn to be actual presuppose\n",
      "\n",
      "thought. what relatively other: a whatever obvious, to understand a proposition means to size of further ages. and expressed in a certain,\n",
      "second, a\n",
      "admit the 2.1515 mathematics are of /v 'spatial introducing clear.\n",
      "\n",
      "alphabet) to be a proposition. remark it 2.01231 if one composite: for what is common to all the\n",
      "5.241 all clear.\n",
      "\n",
      "character--are outside be. draw a use 'exist' musical then the 5.2522 reality, but strictly, essence\n",
      "is to roughly an this boundaries.\n",
      "\n",
      "achieve the 2.161 of sure kind: column as a objects, 4.12 propositions equations.\n",
      "\n",
      "establishing the against establish of the variable'.\n",
      "\n",
      "certainty, 2.1512 it is ourselves.\n",
      "\n",
      "ending with a proposition of me triangles and has. a summed experience. 5.142\n",
      "discloses\n",
      "identity-sign, three is outwards balls was what 5.131 if it is indicate the 4.463 of the much (e)\n",
      "priori--to multiplication, to affix which transcend object, to 5.631 it is humanly possible to\n",
      "fit a term of circumstances--of which natural it mesh, and identical ('thing',\n",
      "\n",
      "(ttff) non-accidental cannot reality, in this 2.1514 itself a 4.11 the explained.\n",
      "\n",
      "nonsensical. seems 2.19 a throw leaving coloured, q)', 2.15 the same 2.063 the 6.2341\n",
      "that is allows signalize\n",
      "with object: of features, false. (p)\n",
      "distinguished\n",
      "2.022 it is tautologies.\n",
      "\n",
      "too, we 6.3 p--or with the schema of the 5.5303 track.)\n",
      "\n",
      "are.)\n",
      "\n",
      "described; notation.\n",
      "\n",
      "sharp value, it must be negation of truth that whitehead problem. of the (and, of\n",
      "presentiment that there must be z, constructed, 6.241 4.1212 by /'/'x, 6.1223 etc.)\n",
      "question? can be eye and the same 6.232\n",
      "mesh, and applying 2.12 a picture is that it 2.034 sheet, class the truth\n",
      "characterizes the 3.343 writing or use it with a peculiar to the conscious of\n",
      "this q)' as 4 it though,\n",
      "that on'.\n",
      "\n",
      "y(o(fx)). type\n"
     ]
    }
   ],
   "source": [
    "# Without Hidden Layer; \n",
    "# # Number of Epochs\n",
    "# num_epochs = 200\n",
    "# # Batch Size\n",
    "# batch_size = 32\n",
    "# # RNN Size\n",
    "# rnn_size = 512\n",
    "# # Embedding Dimension Size\n",
    "# embed_dim = 500\n",
    "# # Sequence Length\n",
    "# seq_length = 15\n",
    "# # Learning Rate\n",
    "# learning_rate = 0.005\n",
    "# # Show stats for every n number of batches\n",
    "# show_every_n_batches = 60\n",
    "\n",
    "\n",
    "gen_length = 500\n",
    "\n",
    "prime_word = 'world'\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(save_dir + '.meta')\n",
    "    loader.restore(sess, save_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    input_text, initial_state, final_state, probs = get_tensors(loaded_graph)\n",
    "\n",
    "    # Sentences generation setup\n",
    "    gen_sentences = [prime_word]\n",
    "#     prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "\n",
    "    # Generate sentences\n",
    "    for n in range(gen_length):\n",
    "        # Dynamic Input\n",
    "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "#         print (dyn_input)\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "#         print (dyn_seq_length)\n",
    "        # Get Prediction\n",
    "#         print (prev_state)\n",
    "        probabilities, prev_state = sess.run([probs, final_state],{input_text: dyn_input, initial_state: prev_state})      \n",
    "#         print (prev_state.shape)\n",
    "#         print (probabilities)\n",
    "        pred_word = pick_word(probabilities[0][dyn_seq_length-1], int_to_vocab)\n",
    "        gen_sentences.append(pred_word)\n",
    "    \n",
    "    # Remove tokens\n",
    "    gen_texts = ' '.join(gen_sentences)\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        gen_texts = gen_texts.replace(' ' + token.lower(), key)\n",
    "    gen_texts = gen_texts.replace('\\n ', '\\n')\n",
    "    gen_texts = gen_texts.replace('( ', '(')\n",
    "\n",
    "    print(gen_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (int_to_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13825,\n",
       " 9343,\n",
       " 4970,\n",
       " 2887,\n",
       " 12205,\n",
       " 11137,\n",
       " 11703,\n",
       " 8429,\n",
       " 14270,\n",
       " 6656,\n",
       " 1439,\n",
       " 9343,\n",
       " 7704,\n",
       " 4342,\n",
       " 4970,\n",
       " 9738,\n",
       " 213,\n",
       " 9789,\n",
       " 9343,\n",
       " 3600,\n",
       " 8931,\n",
       " 12482,\n",
       " 9343,\n",
       " 3251,\n",
       " 11259,\n",
       " 9789,\n",
       " 9343,\n",
       " 2916,\n",
       " 12064,\n",
       " 10552,\n",
       " 13145,\n",
       " 4801,\n",
       " 6285,\n",
       " 9343,\n",
       " 8972,\n",
       " 11259,\n",
       " 12064,\n",
       " 10552,\n",
       " 4772,\n",
       " 2730,\n",
       " 13145,\n",
       " 5637,\n",
       " 360,\n",
       " 11225,\n",
       " 9343,\n",
       " 2916,\n",
       " 12064,\n",
       " 10552,\n",
       " 13145,\n",
       " 5637,\n",
       " 360,\n",
       " 13487,\n",
       " 1519,\n",
       " 1644,\n",
       " 13087,\n",
       " 9008,\n",
       " 9343,\n",
       " 8115,\n",
       " 275,\n",
       " 9343,\n",
       " 4970,\n",
       " 9738,\n",
       " 213,\n",
       " 9343,\n",
       " 6019,\n",
       " 9076,\n",
       " 3865,\n",
       " 10565,\n",
       " 14732,\n",
       " 12382,\n",
       " 14744,\n",
       " 9343,\n",
       " 13936,\n",
       " 10154,\n",
       " 6776,\n",
       " 13985,\n",
       " 1819,\n",
       " 12268,\n",
       " 8016,\n",
       " 12064,\n",
       " 1897,\n",
       " 7220,\n",
       " 1819,\n",
       " 12268,\n",
       " 9343,\n",
       " 825,\n",
       " 12064,\n",
       " 11914,\n",
       " 4772,\n",
       " 11021,\n",
       " 7125,\n",
       " 6367,\n",
       " 12064,\n",
       " 4241,\n",
       " 8336,\n",
       " 3360,\n",
       " 13825,\n",
       " 9343,\n",
       " 4017,\n",
       " 11589,\n",
       " 4360,\n",
       " 11137,\n",
       " 5638,\n",
       " 6849,\n",
       " 2911,\n",
       " 10209,\n",
       " 4360,\n",
       " 9122,\n",
       " 7416,\n",
       " 9343,\n",
       " 2911,\n",
       " 8484,\n",
       " 9122,\n",
       " 682,\n",
       " 7818,\n",
       " 2508,\n",
       " 13205,\n",
       " 9657,\n",
       " 2911,\n",
       " 9343,\n",
       " 9642,\n",
       " 6645,\n",
       " 12268,\n",
       " 1057,\n",
       " 13873,\n",
       " 12064,\n",
       " 12268,\n",
       " 11517,\n",
       " 9343,\n",
       " 13985,\n",
       " 4360,\n",
       " 12268,\n",
       " 12350,\n",
       " 7589,\n",
       " 12064,\n",
       " 1629,\n",
       " 4241,\n",
       " 3284,\n",
       " 2174,\n",
       " 7619,\n",
       " 291,\n",
       " 12268,\n",
       " 9343,\n",
       " 891,\n",
       " 11585,\n",
       " 1183,\n",
       " 1501,\n",
       " 2447,\n",
       " 8484,\n",
       " 9392,\n",
       " 12064,\n",
       " 11817,\n",
       " 2911,\n",
       " 9642,\n",
       " 13825,\n",
       " 2659,\n",
       " 218,\n",
       " 9343,\n",
       " 10371,\n",
       " 9068,\n",
       " 4622,\n",
       " 2911,\n",
       " 4200,\n",
       " 6344,\n",
       " 6645,\n",
       " 12268,\n",
       " 13283,\n",
       " 1057,\n",
       " 9343,\n",
       " 11505,\n",
       " 4360,\n",
       " 9122,\n",
       " 7416,\n",
       " 12064,\n",
       " 5230,\n",
       " 2911,\n",
       " 17,\n",
       " 11053,\n",
       " 7269,\n",
       " 11585,\n",
       " 4360,\n",
       " 3125,\n",
       " 1183,\n",
       " 2174,\n",
       " 9343,\n",
       " 8931,\n",
       " 4772,\n",
       " 6645,\n",
       " 11137,\n",
       " 12656,\n",
       " 891,\n",
       " 8709,\n",
       " 4241,\n",
       " 891,\n",
       " 9343,\n",
       " 9540,\n",
       " 5089,\n",
       " 13941,\n",
       " 12268,\n",
       " 3997,\n",
       " 2931,\n",
       " 9343,\n",
       " 1909,\n",
       " 1409,\n",
       " 9103,\n",
       " 4360,\n",
       " 8336,\n",
       " 14643,\n",
       " 9343,\n",
       " 1813,\n",
       " 5863,\n",
       " 9343,\n",
       " 14409,\n",
       " 10483,\n",
       " 12988,\n",
       " 8145,\n",
       " 3727,\n",
       " 9343,\n",
       " 2952,\n",
       " 2231,\n",
       " 7800,\n",
       " 10960,\n",
       " 1318,\n",
       " 9343,\n",
       " 3126,\n",
       " 9080,\n",
       " 9343,\n",
       " 9390,\n",
       " 9343,\n",
       " 11231,\n",
       " 9343,\n",
       " 925,\n",
       " 13907,\n",
       " 9343,\n",
       " 925,\n",
       " 9120,\n",
       " 9414,\n",
       " 9343,\n",
       " 12206,\n",
       " 4360,\n",
       " 10673,\n",
       " 9858,\n",
       " 9343,\n",
       " 9105,\n",
       " 12702,\n",
       " 9343,\n",
       " 3324,\n",
       " 9523,\n",
       " 223,\n",
       " 3463,\n",
       " 291,\n",
       " 12268,\n",
       " 13285,\n",
       " 4007,\n",
       " 10537,\n",
       " 7405,\n",
       " 9343,\n",
       " 12268,\n",
       " 8167,\n",
       " 2911,\n",
       " 11769,\n",
       " 3353,\n",
       " 291,\n",
       " 11755,\n",
       " 14203,\n",
       " 4935,\n",
       " 2730,\n",
       " 4772,\n",
       " 9343,\n",
       " 6736,\n",
       " 11823,\n",
       " 2911,\n",
       " 4251,\n",
       " 12268,\n",
       " 8167,\n",
       " 10606,\n",
       " 4772,\n",
       " 11490,\n",
       " 3353,\n",
       " 291,\n",
       " 9343,\n",
       " 7677,\n",
       " 12268,\n",
       " 3028,\n",
       " 4684,\n",
       " 5466,\n",
       " 3391,\n",
       " 3178,\n",
       " 8214,\n",
       " 4990,\n",
       " 148,\n",
       " 3725,\n",
       " 9343,\n",
       " 4684,\n",
       " 7151,\n",
       " 26,\n",
       " 4684,\n",
       " 8214,\n",
       " 5164,\n",
       " 12856,\n",
       " 2141,\n",
       " 5115,\n",
       " 4684,\n",
       " 14700,\n",
       " 11356,\n",
       " 12856,\n",
       " 3126,\n",
       " 9343,\n",
       " 1271,\n",
       " 9343,\n",
       " 12702,\n",
       " 291,\n",
       " 3251,\n",
       " 11259,\n",
       " 9343,\n",
       " 12268,\n",
       " 13145,\n",
       " 3236,\n",
       " 4765,\n",
       " 7713,\n",
       " 1819,\n",
       " 12268,\n",
       " 8305,\n",
       " 47,\n",
       " 11137,\n",
       " 899,\n",
       " 2249,\n",
       " 12064,\n",
       " 9343,\n",
       " 3151,\n",
       " 11158,\n",
       " 10274,\n",
       " 4765,\n",
       " 9214,\n",
       " 4360,\n",
       " 12268,\n",
       " 10552,\n",
       " 13937,\n",
       " 12268,\n",
       " 2756,\n",
       " 9343,\n",
       " 7220,\n",
       " 10274,\n",
       " 4765,\n",
       " 13248,\n",
       " 2742,\n",
       " 9343,\n",
       " 4360,\n",
       " 6367,\n",
       " 6607,\n",
       " 3982,\n",
       " 4685,\n",
       " 8947,\n",
       " 4063,\n",
       " 14315,\n",
       " 4126,\n",
       " 8970,\n",
       " 2880,\n",
       " 8515,\n",
       " 6600,\n",
       " 2523,\n",
       " 9343,\n",
       " 5634,\n",
       " 14814,\n",
       " 5368,\n",
       " 6332,\n",
       " 4124,\n",
       " 778,\n",
       " 4816,\n",
       " 12847,\n",
       " 8520,\n",
       " 10809,\n",
       " 4187,\n",
       " 9343,\n",
       " 4360,\n",
       " 6367,\n",
       " 4435,\n",
       " 1098,\n",
       " 10664,\n",
       " 4960,\n",
       " 14430,\n",
       " 10014,\n",
       " 10189,\n",
       " 143,\n",
       " 9343,\n",
       " 12268,\n",
       " 13145,\n",
       " 12064,\n",
       " 12268,\n",
       " 8972,\n",
       " 11259,\n",
       " 3141,\n",
       " 9279,\n",
       " 8880,\n",
       " 3687,\n",
       " 360,\n",
       " 3236,\n",
       " 9343,\n",
       " 4765,\n",
       " 14638,\n",
       " 9343,\n",
       " 10354,\n",
       " 12702,\n",
       " 9343,\n",
       " 836,\n",
       " 9343,\n",
       " 1643,\n",
       " 2886,\n",
       " 10355,\n",
       " 6367,\n",
       " 9963,\n",
       " 12064,\n",
       " 4241,\n",
       " 8309,\n",
       " 13599,\n",
       " 13947,\n",
       " 8484,\n",
       " 14278,\n",
       " 6367,\n",
       " 11130,\n",
       " 9343,\n",
       " 13599,\n",
       " 11659,\n",
       " 2996,\n",
       " 4042,\n",
       " 4772,\n",
       " 12893,\n",
       " 10083,\n",
       " 6224,\n",
       " 6341,\n",
       " 8931,\n",
       " 138,\n",
       " 9343,\n",
       " 13026,\n",
       " 4090,\n",
       " 6771,\n",
       " 10278,\n",
       " 10274,\n",
       " 1361,\n",
       " 11137,\n",
       " 9823,\n",
       " 3656,\n",
       " 12064,\n",
       " 1643,\n",
       " 2174,\n",
       " 4360,\n",
       " 12268,\n",
       " 9343,\n",
       " 4016,\n",
       " 10112,\n",
       " 4077,\n",
       " 2911,\n",
       " 3971,\n",
       " 12064,\n",
       " 6367,\n",
       " 9963,\n",
       " 4772,\n",
       " 8946,\n",
       " 1643,\n",
       " 2174,\n",
       " 4360,\n",
       " 6367,\n",
       " 50,\n",
       " 5637,\n",
       " 9343,\n",
       " 3905,\n",
       " 2408,\n",
       " 6317,\n",
       " 4276,\n",
       " 2925,\n",
       " 9343,\n",
       " 533,\n",
       " 10274,\n",
       " 6341,\n",
       " 291,\n",
       " 10979,\n",
       " 2996,\n",
       " 11089,\n",
       " 1895,\n",
       " 1819,\n",
       " 4424,\n",
       " 4772,\n",
       " 9343,\n",
       " 1323,\n",
       " 13751,\n",
       " 12268,\n",
       " 2965,\n",
       " 12268,\n",
       " 1980,\n",
       " 9371,\n",
       " 710,\n",
       " 12268,\n",
       " 9343,\n",
       " 9621,\n",
       " 9343,\n",
       " 12268,\n",
       " 7220,\n",
       " 12206,\n",
       " 12911,\n",
       " 11137,\n",
       " 8906,\n",
       " 9150,\n",
       " 12268,\n",
       " 3362,\n",
       " 12064,\n",
       " 9293,\n",
       " 4077,\n",
       " 223,\n",
       " 9343,\n",
       " 11659,\n",
       " 13941,\n",
       " 6186,\n",
       " 3125,\n",
       " 6224,\n",
       " 6341,\n",
       " 7829,\n",
       " 10046,\n",
       " 11053,\n",
       " 2554,\n",
       " 4772,\n",
       " 9343,\n",
       " 9472,\n",
       " 9150,\n",
       " 8124,\n",
       " 14000,\n",
       " 6645,\n",
       " 9122,\n",
       " 3905,\n",
       " 6887,\n",
       " 12064,\n",
       " 6116,\n",
       " 14826,\n",
       " 9343,\n",
       " 14237,\n",
       " 291,\n",
       " 2661,\n",
       " 8035,\n",
       " 9343,\n",
       " 4424,\n",
       " 470,\n",
       " 2996,\n",
       " 11051,\n",
       " 11354,\n",
       " 223,\n",
       " 12619,\n",
       " 7469,\n",
       " 9343,\n",
       " 10467,\n",
       " 291,\n",
       " 14206,\n",
       " 10537,\n",
       " 4360,\n",
       " 4241,\n",
       " 13026,\n",
       " 2911,\n",
       " 4360,\n",
       " 11053,\n",
       " 2554,\n",
       " 12064,\n",
       " 138,\n",
       " 3125,\n",
       " 9343,\n",
       " 533,\n",
       " 657,\n",
       " 10859,\n",
       " 9971,\n",
       " 2502,\n",
       " 9343,\n",
       " 533,\n",
       " 223,\n",
       " 9803,\n",
       " 1819,\n",
       " 8291,\n",
       " 12268,\n",
       " 12334,\n",
       " 3397,\n",
       " 12064,\n",
       " 6367,\n",
       " 8007,\n",
       " 4360,\n",
       " 4276,\n",
       " 9343,\n",
       " 1534,\n",
       " 9311,\n",
       " 9343,\n",
       " 8214,\n",
       " 6136,\n",
       " 5674,\n",
       " 6100,\n",
       " 9343,\n",
       " 5466,\n",
       " 5321,\n",
       " 9343,\n",
       " 8214,\n",
       " 4990,\n",
       " 148,\n",
       " 3714,\n",
       " 9343,\n",
       " 5088,\n",
       " 9343,\n",
       " 12268,\n",
       " 9983,\n",
       " 3125,\n",
       " 9963,\n",
       " 13555,\n",
       " 4360,\n",
       " 1643,\n",
       " 3014,\n",
       " 223,\n",
       " 12268,\n",
       " 847,\n",
       " 12064,\n",
       " 9343,\n",
       " 14409,\n",
       " 8579,\n",
       " 3125,\n",
       " 10274,\n",
       " 4638,\n",
       " 3353,\n",
       " 1819,\n",
       " 12268,\n",
       " 4016,\n",
       " 9343,\n",
       " 4451,\n",
       " 3039,\n",
       " 14826,\n",
       " 10679,\n",
       " 7327,\n",
       " 2593,\n",
       " 12268,\n",
       " 5895,\n",
       " 12064,\n",
       " 4347,\n",
       " 9343,\n",
       " 12064,\n",
       " 1125,\n",
       " 12064,\n",
       " 11137,\n",
       " 5416,\n",
       " 12064,\n",
       " 13898,\n",
       " 12268,\n",
       " 5280,\n",
       " 12064,\n",
       " 9343,\n",
       " 5305,\n",
       " 7589,\n",
       " 12064,\n",
       " 9121,\n",
       " 4772,\n",
       " 11053,\n",
       " 14451,\n",
       " 9963,\n",
       " 10274,\n",
       " 11659,\n",
       " 9343,\n",
       " 12647,\n",
       " 13936,\n",
       " 8124,\n",
       " 9983,\n",
       " 10355,\n",
       " 8980,\n",
       " 1897,\n",
       " 3937,\n",
       " 12064,\n",
       " 3125,\n",
       " 4409,\n",
       " 2174,\n",
       " 9343,\n",
       " 944,\n",
       " 11137,\n",
       " 3887,\n",
       " 8076,\n",
       " 6840,\n",
       " 4935,\n",
       " 12268,\n",
       " 11463,\n",
       " 3696,\n",
       " 5398,\n",
       " 9963,\n",
       " 2107,\n",
       " 9343,\n",
       " 13443,\n",
       " 1238,\n",
       " 11137,\n",
       " 11193,\n",
       " 10284,\n",
       " 13666,\n",
       " 10046,\n",
       " 5228,\n",
       " 1535,\n",
       " 291,\n",
       " 11572,\n",
       " 9343,\n",
       " 13599,\n",
       " 3324,\n",
       " 11022,\n",
       " 9150,\n",
       " 3600,\n",
       " 291,\n",
       " 10744,\n",
       " 13936,\n",
       " 4241,\n",
       " 14046,\n",
       " 4360,\n",
       " 11137,\n",
       " 3284,\n",
       " 214,\n",
       " 9343,\n",
       " 7416,\n",
       " 9963,\n",
       " 5518,\n",
       " 3088,\n",
       " 9150,\n",
       " 2302,\n",
       " 12403,\n",
       " 8880,\n",
       " 12268,\n",
       " 7621,\n",
       " 9987,\n",
       " 9343,\n",
       " 13599,\n",
       " 11585,\n",
       " 12268,\n",
       " 9983,\n",
       " 1210,\n",
       " 5786,\n",
       " 10046,\n",
       " 5228,\n",
       " 3199,\n",
       " 291,\n",
       " 1824,\n",
       " 9343,\n",
       " 4360,\n",
       " 11137,\n",
       " 12172,\n",
       " 4119,\n",
       " 4772,\n",
       " 6645,\n",
       " 8381,\n",
       " 9343,\n",
       " 11397,\n",
       " 8189,\n",
       " 5241,\n",
       " 10583,\n",
       " 291,\n",
       " 9110,\n",
       " 3324,\n",
       " 1074,\n",
       " 14046,\n",
       " 6317,\n",
       " 9343,\n",
       " 8021,\n",
       " 11137,\n",
       " 6960,\n",
       " 9963,\n",
       " 7021,\n",
       " 11585,\n",
       " 9963,\n",
       " 1210,\n",
       " 9371,\n",
       " 11819,\n",
       " 12268,\n",
       " 3605,\n",
       " 11585,\n",
       " 9963,\n",
       " 9343,\n",
       " 7984,\n",
       " 9890,\n",
       " 10278,\n",
       " 9371,\n",
       " 13825,\n",
       " 14306,\n",
       " 7269,\n",
       " 14409,\n",
       " 6162,\n",
       " 3324,\n",
       " 9343,\n",
       " 9983,\n",
       " 14237,\n",
       " 4443,\n",
       " 12320,\n",
       " 10083,\n",
       " 9963,\n",
       " 7517,\n",
       " 291,\n",
       " 5546,\n",
       " 14203,\n",
       " 13941,\n",
       " 4360,\n",
       " 9122,\n",
       " 4238,\n",
       " 9343,\n",
       " 13279,\n",
       " 14023,\n",
       " 1645,\n",
       " 12172,\n",
       " 14463,\n",
       " 4241,\n",
       " 8646,\n",
       " 12064,\n",
       " 4465,\n",
       " 9343,\n",
       " 9507,\n",
       " 5637,\n",
       " 12268,\n",
       " 3889,\n",
       " 1060,\n",
       " 12064,\n",
       " 12268,\n",
       " 6450,\n",
       " 1819,\n",
       " 4241,\n",
       " 10980,\n",
       " 9343,\n",
       " 10210,\n",
       " 291,\n",
       " 3056,\n",
       " 3180,\n",
       " 11137,\n",
       " 8639,\n",
       " 3715,\n",
       " 12064,\n",
       " 3066,\n",
       " 12463,\n",
       " 4360,\n",
       " 6738,\n",
       " 13929,\n",
       " 9343,\n",
       " 12268,\n",
       " 14409,\n",
       " 14206,\n",
       " 4360,\n",
       " 4241,\n",
       " 3284,\n",
       " 14258,\n",
       " 10355,\n",
       " 1183,\n",
       " 12979,\n",
       " 11137,\n",
       " 2249,\n",
       " 12064,\n",
       " 9343,\n",
       " 9106,\n",
       " 12064,\n",
       " 12674,\n",
       " 3125,\n",
       " 14237,\n",
       " 9214,\n",
       " 4360,\n",
       " 12268,\n",
       " 11833,\n",
       " 12064,\n",
       " 8124,\n",
       " 8076,\n",
       " 9343,\n",
       " 4772,\n",
       " 5601,\n",
       " 12823,\n",
       " 2479,\n",
       " 9343,\n",
       " 12268,\n",
       " 11463,\n",
       " 2911,\n",
       " 8910,\n",
       " 12268,\n",
       " 11463,\n",
       " 2480,\n",
       " 14237,\n",
       " 11846,\n",
       " 9540,\n",
       " 7938,\n",
       " 9343,\n",
       " 9073,\n",
       " 10046,\n",
       " 2302,\n",
       " 11668,\n",
       " 4772,\n",
       " 8305,\n",
       " 9106,\n",
       " 4225,\n",
       " 3889,\n",
       " 7327,\n",
       " 12064,\n",
       " 9343,\n",
       " 8124,\n",
       " 14237,\n",
       " 9361,\n",
       " 1564,\n",
       " 2911,\n",
       " 11633,\n",
       " 4464,\n",
       " 8484,\n",
       " 13936,\n",
       " 12268,\n",
       " 6546,\n",
       " 9343,\n",
       " 12064,\n",
       " 11137,\n",
       " 6012,\n",
       " 4313,\n",
       " 4772,\n",
       " 14076,\n",
       " 956,\n",
       " 11845,\n",
       " 11137,\n",
       " 2249,\n",
       " 12064,\n",
       " 9343,\n",
       " 6340,\n",
       " 2274,\n",
       " 14237,\n",
       " 14427,\n",
       " 3125,\n",
       " 6847,\n",
       " 6341,\n",
       " 291,\n",
       " 13825,\n",
       " 2710,\n",
       " 4772,\n",
       " 944,\n",
       " 9343,\n",
       " 7829,\n",
       " 14143,\n",
       " 3971,\n",
       " 11585,\n",
       " 10083,\n",
       " 4981,\n",
       " 3532,\n",
       " 9150,\n",
       " 14203,\n",
       " 4981,\n",
       " 7984,\n",
       " 833,\n",
       " 11137,\n",
       " 13426,\n",
       " 12064,\n",
       " 12268,\n",
       " 9343,\n",
       " 11605,\n",
       " 13031,\n",
       " 4241,\n",
       " 3284,\n",
       " 2174,\n",
       " 5074,\n",
       " 6602,\n",
       " 3687,\n",
       " 12491,\n",
       " 9343,\n",
       " 3020,\n",
       " 291,\n",
       " 11137,\n",
       " 1897,\n",
       " 3943,\n",
       " 9512,\n",
       " 9963,\n",
       " 6341,\n",
       " 5074,\n",
       " 5704,\n",
       " 3020,\n",
       " 12268,\n",
       " 10177,\n",
       " 12064,\n",
       " 13726,\n",
       " 9343,\n",
       " 3324,\n",
       " 13026,\n",
       " 4360,\n",
       " 3324,\n",
       " 6255,\n",
       " 1183,\n",
       " 12974,\n",
       " 13250,\n",
       " 291,\n",
       " 13825,\n",
       " 2191,\n",
       " 10046,\n",
       " 3943,\n",
       " 291,\n",
       " 9343,\n",
       " 6059,\n",
       " 5653,\n",
       " 2555,\n",
       " 9963,\n",
       " 13599,\n",
       " 3591,\n",
       " 291,\n",
       " 9162,\n",
       " 11585,\n",
       " 3324,\n",
       " 1074,\n",
       " 10415,\n",
       " 9963,\n",
       " 9343,\n",
       " 6341,\n",
       " 13992,\n",
       " 4360,\n",
       " 5465,\n",
       " 12003,\n",
       " 4772,\n",
       " ...]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
