{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Philosophy Texts by Using RNN\n",
    "\n",
    "This is a project that uses RNN to generate new texts based ont trained texts. This project is based off one of the projects I have done at the Nanodegree program on Udacity. (See link below. Originally the project is about generating texts on Simpsons based on Tensorflow 1.0). A lot of software library packages and training techniques have been updated/deprecated, so I am thinking to reconstruct it in a new notebook, get it running again and see the results now. Although the generated texts are more or less nonsensical (oh well, just like every other philosophy text), based on the word pattern, we can still see how the RNN learns a seq2seq model. \n",
    "\n",
    "Here are some helpful links that on this issue:\n",
    "* Karpathy, The Unreadonable Effectiveness of Recurrent Neural Networks: http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "* Github Repo of Udacity's Deep Learning Nanodegree Foundation program: https://github.com/udacity/deep-learning\n",
    "\n",
    "\n",
    "Here is how different parts of this notebook will be organized:\n",
    "* Pre-processing\n",
    "* Build the network\n",
    "* Training\n",
    "* Generating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "### Check the environments\n",
    "Make sure the Python version is 3.6 and gpu can be detected. (Haven't tested it on lower version of Python.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local/AIUGA/jeremyshi/miniconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.3 |Anaconda, Inc.| (default, Nov 20 2017, 20:41:42) \n",
      "[GCC 7.2.0]\n",
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import sys\n",
    "print (sys.version)\n",
    "print (tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the txt file of the book\n",
    "\n",
    "I've tried different books as the inputs. This time we use Kant's _Critique of Pure Reason_ . You can find the txt version of the book here: http://www.gutenberg.org/ebooks/4280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/kant_CPR.txt', 'r') as file:\n",
    "    book = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lookup Table\n",
    "\n",
    "In order to feed the texts into the neural network (i.e. represent the texts in the network), some important steps of pre-processing include to build a look-up table for each word.  \n",
    "\n",
    "Depending on different ways that we want to build our network, there are other options for pre-precessing as well. Note that here we build a look-up table for each _word_, instead of each _character_. Both are valid choices for building seq2seq models. (Alternatively, I am not aware of whether people have doen seq2seq learning at the sentence level. But that does not seem be too interesting for generative models.) After all, the choice between word and character in building seq2seq models does determine how we encode the representations in the network, as we will see soon in the part of network building. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_up_table(text):\n",
    "    '''\n",
    "    Generate a lookup table for all words in a text. Input should be the pre-processed list of strings(words).\n",
    "    Return two dicts, which contain word:int and int:word as key:value pairs, respectively.\n",
    "    '''\n",
    "    vocab = set(text)\n",
    "    vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "    int_to_vocab = dict(enumerate(vocab))\n",
    "    return vocab_to_int, int_to_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_token_lookup():\n",
    "    '''\n",
    "    Build a token lookup table for special characters we encounter in the text. Used for both pre-processing and generating. \n",
    "    '''\n",
    "    keys = ['.', ',', '\"', ';', '!', '?', '(', ')', '--','\\n'] \n",
    "    values = ['||Period||','||Comma||','||Quotation_Mark||','||Semicolon||','||Exclamation_mark||','||Question_mark||','||Left_Parentheses||','||Right_Parentheses||','||Dash||','||Return||']  \n",
    "    return (dict(zip(keys,values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_pre_processing(book):\n",
    "    '''\n",
    "    Main function of pre-processing. \n",
    "    int_text is the encoded numerical representation of texts that we will use in the neural network.\n",
    "    '''\n",
    "    token_dict = build_token_lookup()\n",
    "    for key, token in token_dict.items():\n",
    "        book = book.replace(key, ' {} '.format(token))\n",
    "    processed_book = book.lower().split()\n",
    "    vocab_to_int, int_to_vocab = look_up_table(processed_book)\n",
    "    int_text = [vocab_to_int[word] for word in processed_book]\n",
    "    return token_dict, vocab_to_int, int_to_vocab, int_text\n",
    "\n",
    "token_dict, vocab_to_int, int_to_vocab, int_text = main_pre_processing(book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check what the encoded text looks like. Here are the first 200 encoded words of the text and the vocabulary size of the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6656, 2468, 2729, 3845, 65, 5025, 5025, 2817, 5545, 5643, 5025, 5025, 561, 2817, 3192, 3449, 2301, 3449, 5886, 3449, 6372, 5025, 5025, 5025, 5025, 5453, 6563, 6656, 301, 4936, 4726, 2198, 5025, 5025, 6291, 65, 4726, 1930, 5467, 715, 2729, 6025, 2328, 4726, 2122, 471, 493, 6563, 4629, 5025, 2944, 4726, 2140, 535, 4696, 5232, 4726, 2869, 5725, 5332, 6933, 2817, 6025, 92, 5025, 874, 4726, 6006, 2140, 535, 4696, 2627, 4726, 2869, 5725, 3450, 6586, 4584, 2729, 5025, 6656, 2125, 3449, 5025, 5025, 535, 3462, 2833, 632, 1170, 1253, 6652, 6975, 2729, 6025, 92, 3449, 535, 6073, 5025, 3143, 3962, 4726, 2140, 4696, 1873, 460, 3143, 1930, 6656, 4816, 2729, 5025, 1606, 4726, 6491, 6656, 1524, 6491, 4170, 2729, 2140, 5332, 4726, 6033, 6656, 6248, 5025, 166, 4726, 1407, 2817, 1606, 3449, 3143, 2569, 3962, 535, 4702, 4726, 1930, 5025, 588, 6563, 6656, 5292, 2729, 6025, 92, 874, 4726, 6563, 4982, 5816, 6491, 5200, 6510, 5025, 379, 3449, 6006, 535, 6244, 3081, 6217, 4726, 1930, 632, 49, 4726, 6025, 4980, 1854, 5025, 4344, 4982, 2010, 4726, 94, 6052, 2944, 5737, 5694, 6563, 3293, 5025, 6997, 6054, 6491, 5835, 535, 1313, 5587, 3990, 6563, 4990, 4162, 6563, 5025, 3962] 7104\n"
     ]
    }
   ],
   "source": [
    "print (int_text[:200], max(int_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Network\n",
    "\n",
    "We build this RNN with LSTM cells. We first initial inputs, targets, and learning rates by using tensorflow's Placeholder function. get_init_cell is the function where we put a sequence of LSTM cells together to form the initial state of the network. These cells will also be used in running a dynamic RNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs():\n",
    "    \"\"\"\n",
    "    Create TF Placeholders for input, targets, and learning rate.\n",
    "    :return: Tuple (input, targets, learning rate)\n",
    "    \"\"\"\n",
    "    input = tf.placeholder(tf.int32, [None, None],name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')  \n",
    "    learning_rate = tf.placeholder(tf.float32, name='learningrate')\n",
    "    input_tuple = (input, targets, learning_rate)\n",
    "    \n",
    "    return input_tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the method _get_init_cell_, there are different options of LSTM cells from Tensorflow for the line here: `lstm = tf.contrib.rnn.LSTMCell(rnn_size)  `. We can use `BasicLSTMCell`, `LayerNormBasicLSTMCell`(which has layer normalization and recurrent dropout), or even `GRUCell` (but I haven't tested on this). For more information on varieties of cells in RNN, see https://www.tensorflow.org/api_docs/python/tf/contrib/rnn\n",
    "\n",
    "In MultiRNNCell, the argument I use is `[lstm]*2`, which means we double the size/layers of the network. People usually choose 2 or 3. It is from \"Tips and Tricks\" for training RNN by Andrej Karpathy. See his README here: https://github.com/karpathy/char-rnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_init_cell(batch_size, rnn_size):\n",
    "    \"\"\"\n",
    "    Create an RNN Cell and initialize it. \n",
    "    :param batch_size: Size of batches\n",
    "    :param rnn_size: Size of RNNs\n",
    "    :return: Tuple (cell, initialize state)\n",
    "    \"\"\"\n",
    "    lstm = tf.contrib.rnn.LSTMCell(rnn_size)  \n",
    "    cell = tf.contrib.rnn.MultiRNNCell([lstm]*2)    \n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    initial_state = tf.identity(initial_state, name = 'initial_state')\n",
    "    \n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get word embeddings\n",
    "The signicance of this part is that we want to encode the representation of the text into the neural network. In the preprocessing part, we have already transform the document into a numerical representation and each word in the vocabulary of the document corresponds to a natural number. However, as we have notice, there are some 7000 words in the vocabulary in total and if we use one-hot encoding (which is what people would usually do in char2char model since there are only 52 upper and lower case letters and even if adding special characters the dimensions still won't be so big.) But it is different fromt the basic unit in the seq2seq model, which is each word in the vocabulary. Thus, we need to abandon one-hot encoding and embed these thousands of words into lower dimensions using Tensorflow's embedding_lookup function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed(input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Embed input data for Tensorflow\n",
    "    :param input_data: TF placeholder for text input.\n",
    "    :param vocab_size: Number of words in vocabulary.\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Embedded input.\n",
    "    \"\"\"\n",
    "    embeddings = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1.0, 1.0))\n",
    "    embed = tf.nn.embedding_lookup(embeddings, input_data)\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build RNN and the whole network\n",
    "\n",
    "Now finally it's time for us to build the network. Unsurprisingly, RNN is the main part of the whole network. We use ` tf.nn.dynamic_rnn` function here. Note that in the Deep Learning Foundation program provided by Udacity, the unittest for this method can only pass in Tensorflow version <1.1, which seems dated now. There is probably something needed to be fixed in the unittest code. However, as I run through this process, it does not really make a difference. As I will show later, we just need to tweak a little thing when running the generative model in the end. \n",
    "\n",
    "And this issue seems never to be solved so far (see: https://github.com/udacity/deep-learning/issues/216)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rnn(cell, inputs):\n",
    "    \"\"\"\n",
    "    Create a RNN using a RNN Cell\n",
    "    :param cell: RNN Cell\n",
    "    :param inputs: Input text data\n",
    "    :return: Tuple (Outputs, Final State)\n",
    "    \"\"\"\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell,  inputs, dtype=tf.float32)\n",
    "    final_state = tf.identity(final_state, name = 'final_state')\n",
    "    return outputs, final_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nn(cell, rnn_size, input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Build part ofRNN cell\n",
    "    :param rnn_size: Size of rnns\n",
    "    :param input_data: Input data\n",
    "    :param vocab_size the neural network\n",
    "    :param cell: : Vocabulary size\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Tuple (Logits, FinalState)\n",
    "    \"\"\"\n",
    "    \n",
    "    input_data = get_embed(input_data, vocab_size, rnn_size)\n",
    "    outputs, final_state = build_rnn(cell, input_data)\n",
    "    logits = tf.contrib.layers.fully_connected (outputs, vocab_size)\n",
    "    return logits, final_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to get batches. This is for running the neural network. As I have mentioned before, we are building a sequence to sequence model here. In particular, we should map a certain length (determined by `seq_length`) of sequences of words to a sequences of words with the same length right after. Depending on the batch_size, the shapes vary. Here is a good example directly provided in the original Udacity notebook -- `get_batches([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], 3, 2)` would return a Numpy array of the following:\n",
    "```\n",
    "[\n",
    "  # First Batch\n",
    "  [\n",
    "    # Batch of Input\n",
    "    [[ 1  2], [ 7  8], [13 14]]\n",
    "    # Batch of targets\n",
    "    [[ 2  3], [ 8  9], [14 15]]\n",
    "  ]\n",
    "\n",
    "  # Second Batch\n",
    "  [\n",
    "    # Batch of Input\n",
    "    [[ 3  4], [ 9 10], [15 16]]\n",
    "    # Batch of targets\n",
    "    [[ 4  5], [10 11], [16 17]]\n",
    "  ]\n",
    "\n",
    "  # Third Batch\n",
    "  [\n",
    "    # Batch of Input\n",
    "    [[ 5  6], [11 12], [17 18]]\n",
    "    # Batch of targets\n",
    "    [[ 6  7], [12 13], [18  1]]\n",
    "  ]\n",
    "]\n",
    "```\n",
    "\n",
    "Notice that in the third batch, the last element of _Batch of Targets_ is 1, which is the same with the first element in the _Batch of Input_ for the first batch. Because if we map a sequence of words to the another sequence of words right after, we will encounter problems for the last batch. Thus we map it back to the first element (taken care of by the line `ydata[-1] = xdata[0]  `) Alternatively, we can just ignore the last sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(int_text, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    Return batches of input and target\n",
    "    :param int_text: Text with the words replaced by their ids\n",
    "    :param batch_size: The size of batch\n",
    "    :param seq_length: The length of sequence\n",
    "    :return: Batches as a Numpy array\n",
    "    \"\"\"\n",
    "    characters_per_batch = batch_size * seq_length\n",
    "    n_batches = len(int_text)//characters_per_batch\n",
    "    \n",
    "    xdata = np.array(int_text[: n_batches * characters_per_batch])\n",
    "    ydata = np.array(int_text[1: n_batches * characters_per_batch + 1])\n",
    "\n",
    "    x_batches = np.split(xdata.reshape(batch_size, -1), n_batches, 1)\n",
    "    y_batches = np.split(ydata.reshape(batch_size, -1), n_batches, 1)\n",
    "    \n",
    "    ydata[-1] = xdata[0]  \n",
    " \n",
    "    return np.array(list(zip(x_batches, y_batches)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Setting the parameters for training here. As usual, if we lower the batch size, the training takes longer. But it is not the only important factor that determines the time and performance. `rnn_size` is another important one. I set `num_epochs` to be 150 in this setting because the loss function gets steady after 100 or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "num_epochs = 150\n",
    "# Batch Size\n",
    "batch_size = 32\n",
    "# RNN Size\n",
    "rnn_size = 512\n",
    "# Embedding Dimension Size\n",
    "embed_dim = 400\n",
    "# Sequence Length\n",
    "seq_length = 40\n",
    "# Learning Rate\n",
    "learning_rate = 0.003\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Graph\n",
    "\n",
    "One important thing in building this graph is the softmax function. It takes the logits as the argument and output the probabilities for word selection. \n",
    "\n",
    "A common type of problem in training a vanilla RNN is the vanishing/exploding gradient. LSTM can usually help alleviate these issues. For even better results, we apply Gradient Clipping on the Adam Optimizer (mainly to avoid exploding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib import seq2seq\n",
    "\n",
    "training = tf.Graph()\n",
    "with training.as_default():\n",
    "    vocab_size = len(int_to_vocab)\n",
    "    input_text, targets, lr = get_inputs()\n",
    "    input_data_shape = tf.shape(input_text)\n",
    "    cell, initial_state = get_init_cell(input_data_shape[0], rnn_size)\n",
    "    logits, final_state = build_nn(cell, rnn_size, input_text, vocab_size, embed_dim)\n",
    "\n",
    "    # Probabilities for generating words\n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "\n",
    "    # Loss function\n",
    "    cost = seq2seq.sequence_loss(logits, targets, tf.ones([input_data_shape[0], input_data_shape[1]]))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the model\n",
    "\n",
    "Time to run the model! We run the model and save it locally (for the purpose the generating texts later). Usually, the best I have done so far is to lower the loss to around 2.5. The optimal loss value also depends on the sizes of the texts and the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Batch:  0 / 205 train sequence loss:  8.868579\n",
      "Epoch:  1 Batch:  95 / 205 train sequence loss:  6.013385\n",
      "Epoch:  2 Batch:  190 / 205 train sequence loss:  5.3977532\n",
      "Epoch:  4 Batch:  80 / 205 train sequence loss:  5.1920447\n",
      "Epoch:  5 Batch:  175 / 205 train sequence loss:  4.935922\n",
      "Epoch:  7 Batch:  65 / 205 train sequence loss:  4.8224144\n",
      "Epoch:  8 Batch:  160 / 205 train sequence loss:  4.8773246\n",
      "Epoch:  10 Batch:  50 / 205 train sequence loss:  4.496931\n",
      "Epoch:  11 Batch:  145 / 205 train sequence loss:  4.5065074\n",
      "Epoch:  13 Batch:  35 / 205 train sequence loss:  4.35735\n",
      "Epoch:  14 Batch:  130 / 205 train sequence loss:  4.214121\n",
      "Epoch:  16 Batch:  20 / 205 train sequence loss:  3.9536355\n",
      "Epoch:  17 Batch:  115 / 205 train sequence loss:  4.0311327\n",
      "Epoch:  19 Batch:  5 / 205 train sequence loss:  3.776989\n",
      "Epoch:  20 Batch:  100 / 205 train sequence loss:  3.8227055\n",
      "Epoch:  21 Batch:  195 / 205 train sequence loss:  3.7022438\n",
      "Epoch:  23 Batch:  85 / 205 train sequence loss:  3.6881828\n",
      "Epoch:  24 Batch:  180 / 205 train sequence loss:  3.6750748\n",
      "Epoch:  26 Batch:  70 / 205 train sequence loss:  3.5594144\n",
      "Epoch:  27 Batch:  165 / 205 train sequence loss:  3.3116632\n",
      "Epoch:  29 Batch:  55 / 205 train sequence loss:  3.465444\n",
      "Epoch:  30 Batch:  150 / 205 train sequence loss:  3.3813171\n",
      "Epoch:  32 Batch:  40 / 205 train sequence loss:  3.3946335\n",
      "Epoch:  33 Batch:  135 / 205 train sequence loss:  3.3753543\n",
      "Epoch:  35 Batch:  25 / 205 train sequence loss:  3.2440555\n",
      "Epoch:  36 Batch:  120 / 205 train sequence loss:  3.2354112\n",
      "Epoch:  38 Batch:  10 / 205 train sequence loss:  3.2865982\n",
      "Epoch:  39 Batch:  105 / 205 train sequence loss:  3.2242236\n",
      "Epoch:  40 Batch:  200 / 205 train sequence loss:  3.140813\n",
      "Epoch:  42 Batch:  90 / 205 train sequence loss:  3.2471638\n",
      "Epoch:  43 Batch:  185 / 205 train sequence loss:  3.2479484\n",
      "Epoch:  45 Batch:  75 / 205 train sequence loss:  3.0695593\n",
      "Epoch:  46 Batch:  170 / 205 train sequence loss:  2.9772651\n",
      "Epoch:  48 Batch:  60 / 205 train sequence loss:  3.0390391\n",
      "Epoch:  49 Batch:  155 / 205 train sequence loss:  3.1523356\n",
      "Epoch:  51 Batch:  45 / 205 train sequence loss:  3.0197613\n",
      "Epoch:  52 Batch:  140 / 205 train sequence loss:  3.2369156\n",
      "Epoch:  54 Batch:  30 / 205 train sequence loss:  2.9286537\n",
      "Epoch:  55 Batch:  125 / 205 train sequence loss:  2.866245\n",
      "Epoch:  57 Batch:  15 / 205 train sequence loss:  3.1168609\n",
      "Epoch:  58 Batch:  110 / 205 train sequence loss:  3.091483\n",
      "Epoch:  60 Batch:  0 / 205 train sequence loss:  2.8343005\n",
      "Epoch:  61 Batch:  95 / 205 train sequence loss:  3.0064528\n",
      "Epoch:  62 Batch:  190 / 205 train sequence loss:  3.1007812\n",
      "Epoch:  64 Batch:  80 / 205 train sequence loss:  3.1067665\n",
      "Epoch:  65 Batch:  175 / 205 train sequence loss:  2.892351\n",
      "Epoch:  67 Batch:  65 / 205 train sequence loss:  2.959169\n",
      "Epoch:  68 Batch:  160 / 205 train sequence loss:  3.2325695\n",
      "Epoch:  70 Batch:  50 / 205 train sequence loss:  3.0446966\n",
      "Epoch:  71 Batch:  145 / 205 train sequence loss:  2.9811637\n",
      "Epoch:  73 Batch:  35 / 205 train sequence loss:  2.8753047\n",
      "Epoch:  74 Batch:  130 / 205 train sequence loss:  2.9707305\n",
      "Epoch:  76 Batch:  20 / 205 train sequence loss:  2.8232641\n",
      "Epoch:  77 Batch:  115 / 205 train sequence loss:  2.9362702\n",
      "Epoch:  79 Batch:  5 / 205 train sequence loss:  2.8129067\n",
      "Epoch:  80 Batch:  100 / 205 train sequence loss:  2.8698277\n",
      "Epoch:  81 Batch:  195 / 205 train sequence loss:  2.909651\n",
      "Epoch:  83 Batch:  85 / 205 train sequence loss:  2.9899678\n",
      "Epoch:  84 Batch:  180 / 205 train sequence loss:  3.079751\n",
      "Epoch:  86 Batch:  70 / 205 train sequence loss:  2.997043\n",
      "Epoch:  87 Batch:  165 / 205 train sequence loss:  2.8017378\n",
      "Epoch:  89 Batch:  55 / 205 train sequence loss:  3.050435\n",
      "Epoch:  90 Batch:  150 / 205 train sequence loss:  2.9197817\n",
      "Epoch:  92 Batch:  40 / 205 train sequence loss:  2.927957\n",
      "Epoch:  93 Batch:  135 / 205 train sequence loss:  3.0701828\n",
      "Epoch:  95 Batch:  25 / 205 train sequence loss:  2.9324503\n",
      "Epoch:  96 Batch:  120 / 205 train sequence loss:  2.9063938\n",
      "Epoch:  98 Batch:  10 / 205 train sequence loss:  3.0073533\n",
      "Epoch:  99 Batch:  105 / 205 train sequence loss:  2.9238956\n",
      "Epoch:  100 Batch:  200 / 205 train sequence loss:  2.9974918\n",
      "Epoch:  102 Batch:  90 / 205 train sequence loss:  3.0719895\n",
      "Epoch:  103 Batch:  185 / 205 train sequence loss:  3.1495533\n",
      "Epoch:  105 Batch:  75 / 205 train sequence loss:  2.929936\n",
      "Epoch:  106 Batch:  170 / 205 train sequence loss:  2.8446004\n",
      "Epoch:  108 Batch:  60 / 205 train sequence loss:  2.9386115\n",
      "Epoch:  109 Batch:  155 / 205 train sequence loss:  3.012483\n",
      "Epoch:  111 Batch:  45 / 205 train sequence loss:  2.9126563\n",
      "Epoch:  112 Batch:  140 / 205 train sequence loss:  3.1213307\n",
      "Epoch:  114 Batch:  30 / 205 train sequence loss:  2.853413\n",
      "Epoch:  115 Batch:  125 / 205 train sequence loss:  2.794526\n",
      "Epoch:  117 Batch:  15 / 205 train sequence loss:  3.084519\n",
      "Epoch:  118 Batch:  110 / 205 train sequence loss:  3.0178237\n",
      "Epoch:  120 Batch:  0 / 205 train sequence loss:  2.7666306\n",
      "Epoch:  121 Batch:  95 / 205 train sequence loss:  2.950029\n",
      "Epoch:  122 Batch:  190 / 205 train sequence loss:  2.99126\n",
      "Epoch:  124 Batch:  80 / 205 train sequence loss:  3.035646\n",
      "Epoch:  125 Batch:  175 / 205 train sequence loss:  2.884969\n",
      "Epoch:  127 Batch:  65 / 205 train sequence loss:  2.9128633\n",
      "Epoch:  128 Batch:  160 / 205 train sequence loss:  3.1574838\n",
      "Epoch:  130 Batch:  50 / 205 train sequence loss:  3.026366\n",
      "Epoch:  131 Batch:  145 / 205 train sequence loss:  2.978301\n",
      "Epoch:  133 Batch:  35 / 205 train sequence loss:  2.8014593\n",
      "Epoch:  134 Batch:  130 / 205 train sequence loss:  2.8932862\n",
      "Epoch:  136 Batch:  20 / 205 train sequence loss:  2.7783377\n",
      "Epoch:  137 Batch:  115 / 205 train sequence loss:  2.9154134\n",
      "Epoch:  139 Batch:  5 / 205 train sequence loss:  2.7974377\n",
      "Epoch:  140 Batch:  100 / 205 train sequence loss:  2.883678\n",
      "Epoch:  141 Batch:  195 / 205 train sequence loss:  2.9011045\n",
      "Epoch:  143 Batch:  85 / 205 train sequence loss:  2.9465625\n",
      "Epoch:  144 Batch:  180 / 205 train sequence loss:  3.0798228\n",
      "Epoch:  146 Batch:  70 / 205 train sequence loss:  2.9879098\n",
      "Epoch:  147 Batch:  165 / 205 train sequence loss:  2.7772717\n",
      "Epoch:  149 Batch:  55 / 205 train sequence loss:  3.0656915\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "batches = get_batches(int_text, batch_size, seq_length)\n",
    "\n",
    "with tf.Session(graph=training) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(num_epochs):\n",
    "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "        for batch_i, (x, y) in enumerate(batches):\n",
    "            feed = {input_text: x, targets: y, initial_state: state, lr: learning_rate}\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
    "                print('Epoch: ', epoch_i, 'Batch: ', batch_i,'/', len(batches), 'train sequence loss: ', train_loss)\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_dir = './save'\n",
    "    saver.save(sess, save_dir)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Texts\n",
    "### Helper functions\n",
    "\n",
    "There are two helper functions for generating texts. `get_tensors` is to load the saved graph by Tensorflow. What we in the saved graph are input, initial_state, final_state, and probabilities, which are all from the results of running the RNN. \n",
    "\n",
    "The method `pick_word` is for picking another word after a word is chosen. And the distribution for the words to be selected is based on the probabilities of the next word. We choose the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensors(loaded_graph):\n",
    "    \"\"\"\n",
    "    Get input, initial state, final state, and probabilities tensor from <loaded_graph>\n",
    "    :param loaded_graph: TensorFlow graph loaded from file\n",
    "    :return: Tuple (InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)\n",
    "    \"\"\"\n",
    "    input_tensor =  loaded_graph.get_tensor_by_name(\"input:0\")\n",
    "    i_s_tensor =  loaded_graph.get_tensor_by_name(\"initial_state:0\")\n",
    "    f_s_tensor =  loaded_graph.get_tensor_by_name(\"final_state:0\")\n",
    "    probs_tensor =  loaded_graph.get_tensor_by_name(\"probs:0\")\n",
    "    print (i_s_tensor, f_s_tensor)\n",
    "    return input_tensor, i_s_tensor, f_s_tensor, probs_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_word(probabilities, int_to_vocab):\n",
    "    \"\"\"\n",
    "    Pick the next word in the generated text\n",
    "    :param probabilities: Probabilites of the next word\n",
    "    :param int_to_vocab: Dictionary of word ids as the keys and words as the values\n",
    "    :return: String of the predicted word\n",
    "    \"\"\"\n",
    "    predicted = np.random.choice(list(int_to_vocab.values()), 1, p = probabilities)[0]\n",
    "    return predicted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Process\n",
    "\n",
    "Now it is the time for us to initiate a tf session again and generate predicated texts. We randomly select some words from the dictionary as the prime word (which is the beginning of the generated text). Based on the sequence length and the trained model, we feed input into the session. When picking a predicted word, for Tensorflow version >1.1, we need to have `pred_word = pick_word(probabilities[0][dyn_seq_length-1], int_to_vocab)`, instead of `pred_word = pick_word(probabilities[dyn_seq_length-1], int_to_vocab)` (which is the line Udacity originally has.) We need to add `[0]` for the probabilities tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The chosen prime word is  substitution\n",
      "INFO:tensorflow:Restoring parameters from ./save\n",
      "Tensor(\"initial_state:0\", shape=(2, 2, ?, 512), dtype=float32) Tensor(\"final_state:0\", shape=(2, 2, ?, 512), dtype=float32)\n",
      "substitution of truth, but\n",
      "attributed to the use of reluctant cases, that friends and refuses,\n",
      "is it more preception secondly, is representative into the\n",
      "he-goat, but observers of a rash and intelligibilis, to subject's all\n",
      "the expansion which excuse to unconnected with in our\n",
      "chart of the forlorn and ball of approximate, but to drives it\n",
      "changed, the principle of which costs from shield it\n",
      "enjoyed. a impassable trademark/copyright datum speciosa, and that, the still key in our\n",
      "differences succeed + descendants by the sense in time. reliable.\n",
      "impetus learner must be subject, therefore, to be remain pretty matters not\n",
      "on it, as the variable, and not nature by steeled, the\n",
      "remarkably polysyllogistica, which is insignia of a merely 4557 appeals to an\n",
      "comprehensibility of a equitable s/he to purpose in meant(a infinitude of all\n",
      "external that can be standpoint possidentis for us; and the antecedes\n",
      "which the determination of a happened in time political to an needless of\n",
      "conveyed transcendentalism, and, by the anthropomorphic treason obviate that deficient\n",
      "in its adopted, but the scholars and by sophist, designation to the\n",
      "hate of occurs, and the restricting state of flight the contemplated anomaly\n",
      "potentially utterly answering a lapse mine, which champion and theorems-- teaching\n",
      "centuries to dog hypothetical directing, we triangle apprehended only\n",
      "by deeply varietates old to the genealogy self-defence of the magnet,\n",
      "or subreptions are at doing employs in the examine of this extensive otherwise than phenomena.\n",
      "\n",
      "the reported affect time to semblance of vague\n",
      "experience in his primarily, and of partly outbreak solar from the\n",
      "avow of inertiae exemplifies absoluta; and thus rendering the admits of locke,\n",
      "as finality of a given usurpation, that is to erroneously\n",
      "merit, in the mediately of these well-grounded directs us to infancy.\n",
      "for this result, in monas, base inevitably fancy to be\n",
      "gbnewby@pglaf. visit: altered, there does improvising approximates midst to a\n",
      "reverti, life preparation for his own inquiry. the\n",
      "images of dismiss the land that gross to introducing into the genealogy\n",
      "stepping the purpose in grossly disturb covertly. perverted when we interested(mistake\n",
      "equal seducing of itself, or turbulent and deeply), the mind's desires of\n",
      "divisibility, approximating in a priori: that by this reveals, watch from a\n",
      "convenience hesitate, come aim of conceal the stationary in the juggling; but we admixtures\n",
      "only from our seas distinctness our connect official discoveries. but there\n",
      "is borrowed from justify uttering ignoratio its a drawing, but phantom, to its purposeless\n",
      "dictation and to synonymous; merits, dissipate the zeno of preclude the\n",
      "lake misuse of the phenomena of the epochs of the deal\n",
      "of my fairly mathematician, the abstracto unphilosophical, and peculiarities: if we have\n",
      "rational-synthetical the change guarding by intimidation, to discloses, describing by the manner of each\n",
      "concluded: of technique of thought, we intelligibility to internal conditioned-- a\n",
      "down to conduce to the opposites of nature. on the bounded expressed:\n",
      "mob of execution from the contributions and risk of spiritualism.\n",
      "it is only\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "gen_length = 600\n",
    "\n",
    "prime_word = int_to_vocab[np.random.choice(list(vocab_to_int.values()))]\n",
    "print ('The chosen prime word is ', prime_word)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(save_dir + '.meta')\n",
    "    loader.restore(sess, save_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    input_text, initial_state, final_state, probs = get_tensors(loaded_graph)\n",
    "\n",
    "    # Sentences generation setup\n",
    "    gen_sentences = [prime_word]\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "    # Generate sentences\n",
    "    for n in range(gen_length):\n",
    "        # Dynamic Input\n",
    "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "        # Get Prediction\n",
    "        probabilities, prev_state = sess.run([probs, final_state],{input_text: dyn_input, initial_state: prev_state})      \n",
    "        pred_word = pick_word(probabilities[0][dyn_seq_length-1], int_to_vocab)\n",
    "        gen_sentences.append(pred_word)    \n",
    "    # Remove tokens\n",
    "    gen_texts = ' '.join(gen_sentences)\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        gen_texts = gen_texts.replace(' ' + token.lower(), key)\n",
    "    gen_texts = gen_texts.replace('\\n ', '\\n')\n",
    "    gen_texts = gen_texts.replace('( ', '(')\n",
    "\n",
    "    print(gen_texts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
